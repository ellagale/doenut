{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worked Example on Solar Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Organic photovoltaic (OPV) are ‘plastic’ solar cells that can be made cheaply and easily as you can use techniques like roll to roll printing, inject printing and spray coating. Current generation solar cells take several years of use before they payback the energy required in their manufacture, OPVs are so efficient that their energy payback is only 24hours. Power conversion efficiencies (PCEs) of OPVs are now around 14%. To commercialise them, we need to figure out how best to manufacture them.\n",
    "\n",
    "Organic photovoltaic devices have a sandwich architecture. The bottom layers Al/Mg and LiF are the bottom electrode. The important part is the bulk hetereojunction, shown in red in the figure below, which comprises of a low band gap polymer which is the electron donor and fullerene which is the electron acceptor. Addition of an additive helps with forming and bridging separate nanodomains of donor and accceptor. Solar cells work by using light to form an exciton which then separates into an electron-hole pair and you want these to be separated from each other, which is why you want separate nanodomain of donor and acceptor. The top of the solar cell is PEDOT:PSS (a conducting polymer) and ITO  (indium tin oxide), a see-through electrode, which together act as the top electrode. \n",
    "\n",
    "![Test_HetJunc.jpeg](Test_HetJunc.jpeg)\n",
    "\n",
    "Figure 1. (a) Schematic of single junction organic photovoltaic (OPV) devices, showing the bulk heterojunction (BHJ; in red), and the multiple interfacial layers in the device. (b) Schematic of BHJ morphology: in this case, a low band gap polymer donor and a fullerene acceptor undergoing nanoscale phase segregation into discrete nanoscale domains of donor and acceptor. The use of an additive is often purported to assist in nanodomain formation, as shown here. Taken from [ACS Nano 2018, 12, 7434−7444]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The task\n",
    "\n",
    "The task is to optimise the construction of this type of solar cell. Donor weight percentage is a measure of the ratio of donor to acceptor in the heterojunction.  Total solution concentration is the concentration of the spin-coating solution. Bulk heterojunction spin-case speed is a measure of how fast you spin the device when coating it with the bulk heterojunction mixture. Processing additive is the amount of additive (diiodooctane) added to the mixture. The thickness of a spun film is determined by the spin speed, solvent vapour pressure and solution viscosity, as both the donor weight percentage and total solution concentration can affect viscosity, the first three factors can all affect the thickness of the final BHJ layer. The additive (diiodooctane) increases the drying time for the film, helping to separate the hetereojunction out into nanodomains of donor and acceptor rich areas. \n",
    "\n",
    "\n",
    "**Factors selected:**\n",
    "\n",
    "| Name  |   Factors                         |Factor range   | No. of levels  |\n",
    "|-------|-----------------------------------|---------------|----------------|\n",
    "| Donor | Donor weight percentage          | 10-55 (wt %)         | 4       |\n",
    "| Conc. | Total solution concentration     | 10-25 (mg/mL)        | 4       |\n",
    "| Spin  | Bulk heterojunction spin-case speed | 600 - 3000  (rpm) | 4       |\n",
    "| Add.  | Processing additive              |  0-12 (vol %)        | 4       | \n",
    "\n",
    "We shall use the shortened names from the table above. \n",
    "\n",
    "**Files: **\n",
    "\n",
    "1. `solar_cells_1.csv` results from the first experiment, fractional factorial, 4 factors and 4 levels, here we have 16 experiments, one failed to solidify. \n",
    "2. `'solar_cells_2.csv'` has results from the second experiment, a fractional factorial, 3 factors and 3 levels. This covers a smaller range. \n",
    "\n",
    "The data is taken from: \n",
    "\"How To Optimize Materials and Devices via Design of Experiments and Machine Learning: Demonstration Using Organic Photovoltaics\", Bing Cao, Lawrence A. Adutwum, Anton O. Oliynyk, Erik J. Luber, Brian C. Olsen, Arthur Mar, and Jillian M. Buriak, ACS Nano 2018, 12, 7434−7444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# for pictures\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for maths\n",
    "import numpy as np\n",
    "\n",
    "import doenut\n",
    "\n",
    "# set the log level\n",
    "import logging\n",
    "doenut.set_log_level(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the first experiment's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/solar_cells_1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up input and response dataframes\n",
    "\n",
    "We must drop the last experiment, as these devices didn't set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.DataFrame(\n",
    "    {\n",
    "        \"Donor %\": [float(x) for x in df.iloc[1:-1, 1]],\n",
    "        \"Conc.\": [float(x) for x in df.iloc[1:-1, 2]],\n",
    "        \"Spin\": [float(x) for x in df.iloc[1:-1, 3]],\n",
    "        \"Add.\": [float(x) for x in df.iloc[1:-1, 4]],\n",
    "    }\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = pd.DataFrame({\"PCE\": [float(x) for x in df[\"PCE\"][1:-1]]})\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Create a linear (main factors only) model\n",
    "\n",
    "Create a linear model, i.e. a model that has just the main effects (also known as a first order model or main effects model) Fit your linear model to the first experiment’s data and calculate R2 and Q2 for your fitted model. Then answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataset object linking the inputs and responses.\n",
    "data_set = doenut.data.ModifiableDataSet(inputs, responses)\n",
    "\n",
    "model = doenut.models.AveragedModel(data_set)\n",
    "\n",
    "r2, q2 = model.r2, model.q2\n",
    "\n",
    "print(f\"R2 is {r2}, Q2 is {q2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Create a quadratic, parsimonious and hierarchical model\n",
    "\n",
    "Create a **quadratic**, **parsimonious** and **hierarchical** model. Starting with a quadratic model, and making sure that all models are hierarchical, optimise the model by removing **only** the **statistically insignificant** terms. Keep a note of the terms removed and teh $R^2$ and $Q^2$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must expand the input dataframe to include the higher order terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_source_list = []\n",
    "source_list = []\n",
    "sat_inputs_orig, sat_source_list = doenut.add_higher_order_terms(\n",
    "    inputs, add_squares=True, add_interactions=True, column_list=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully saturated quadratic model:\n",
    "\n",
    "This contains all the main terms and all the square terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we only want to select some columns and we want it to scale each column so the values are normalised\n",
    "# across columns.\n",
    "# First make a list of the columns we want\n",
    "input_selector = [\n",
    "    \"Donor %\",\n",
    "    \"Conc.\",\n",
    "    \"Spin\",\n",
    "    \"Add.\",\n",
    "    \"Donor %**2\",\n",
    "    \"Conc.**2\",\n",
    "    \"Spin**2\",\n",
    "    \"Add.**2\",\n",
    "]\n",
    "# Note we could also use list indices like this:\n",
    "# input_selector = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "dataset = (\n",
    "    doenut.data.ModifiableDataSet(sat_inputs_orig, responses)\n",
    "    .filter(input_selector)\n",
    "    .scale()\n",
    ")\n",
    "\n",
    "model = doenut.models.AveragedModel(\n",
    "    dataset, scale_run_data=True, drop_duplicates=\"no\"\n",
    ")\n",
    "\n",
    "r2, q2 = model.r2, model.q2\n",
    "\n",
    "print(f\"R2 is {r2}, Q2 is {q2}\")\n",
    "doenut.plot.plot_summary_of_fit_small(r2, q2)\n",
    "doenut.plot.coeff_plot(\n",
    "    model.coeffs,\n",
    "    labels=list(dataset.get().inputs.columns),\n",
    "    errors=\"p95\",\n",
    "    normalise=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's not very good. Let's try with a different label selection, removing the most obviously insignificant terms\n",
    "input_selector = [0, 1, 2, 4, 5, 6]\n",
    "dataset = (\n",
    "    doenut.data.ModifiableDataSet(sat_inputs_orig, responses)\n",
    "    .filter(input_selector)\n",
    "    .scale()\n",
    ")\n",
    "\n",
    "model = doenut.models.AveragedModel(\n",
    "    dataset, scale_run_data=True, drop_duplicates=\"no\"\n",
    ")\n",
    "\n",
    "r2, q2 = model.r2, model.q2\n",
    "\n",
    "print(f\"R2 is {r2}, Q2 is {q2}\")\n",
    "doenut.plot.plot_summary_of_fit_small(r2, q2)\n",
    "doenut.plot.coeff_plot(\n",
    "    model.coeffs,\n",
    "    labels=list(dataset.get().inputs.columns),\n",
    "    errors=\"p95\",\n",
    "    normalise=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 datapoints so 14 DoF.\n",
    "\n",
    "Starting from quadratic model\n",
    "\n",
    "| No. of terms | DoF   | term removed | factor removed | $R^2$  |  $Q^2$ |\n",
    "|--------------|-------|--------------|----------------|--------|--------|\n",
    "| 8            | 6     |              |                | 0.815  | -0.176 |\n",
    "| 7            | 7     | 8            | `'Add.**2`     | 0.813  | 0.0863 |\n",
    "| 6            | 8     | 3            | `Add.`         | 0.813  | 0.332  |\n",
    "\n",
    "this is the model with no statistically insignificant terms. It's heirarchical. \n",
    "\n",
    "The Q2 is better than the main effects only model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Create a parsimonious interaction model\n",
    "\n",
    "Create hierarchical parsimonious interaction model. Starting with a interaction model, and making sure that all models are hierarchical, optimise the model by removing only the statistically insignificant terms. Keep a note of the terms removed and the $Q^2$ and $R^2$ values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_selector = [0, 1, 2, 3, 9, 10]\n",
    "\n",
    "dataset = (\n",
    "    doenut.data.ModifiableDataSet(sat_inputs_orig, responses)\n",
    "    .filter(input_selector)\n",
    "    .scale()\n",
    ")\n",
    "\n",
    "model = doenut.models.AveragedModel(\n",
    "    dataset, scale_run_data=True, drop_duplicates=\"no\"\n",
    ")\n",
    "\n",
    "r2, q2 = model.r2, model.q2\n",
    "\n",
    "print(f\"R2 is {r2}, Q2 is {q2}\")\n",
    "doenut.plot.plot_summary_of_fit_small(r2, q2)\n",
    "doenut.plot.coeff_plot(\n",
    "    model.coeffs,\n",
    "    labels=list(dataset.get().inputs.columns),\n",
    "    errors=\"p95\",\n",
    "    normalise=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "15 terms so 14 DoF\n",
    "\n",
    "Starting from square model\n",
    "\n",
    "| No. of terms | DoF   | term removed | factor removed | $R^2$  |  $Q^2$ |\n",
    "|--------------|-------|--------------|----------------|--------|--------|\n",
    "| 10           | 4     |              |                | 0.811  |  -1.79 |\n",
    "| 9            | 5     |      12      | `Conc.*Add.`   | 0.813  | 0.0863 |\n",
    "| 8            | 6     |      13      | `'Spin*Add.'`  | 0.798  | -0.555 |\n",
    "| 7            | 7     |      11      | `Conc*Spin.`   | 0.777  | 0.0133 |\n",
    "| 6            | 8     |      8       | `Donor*Conc.`  | 0.760  | 0.310  | \n",
    "\n",
    "this is the model with no statistically insignificant terms. It's heirarchical. \n",
    "\n",
    "The Q2 is better than the main effects only model, but not as good as the square terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Combine data from both experiments and train a parsimonious model\n",
    "### Loading the second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/solar_cells_2.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2 = pd.DataFrame(\n",
    "    {\n",
    "        \"Donor %\": [float(x) for x in df.iloc[1:-1, 1]],\n",
    "        \"Conc.\": [float(x) for x in df.iloc[1:-1, 2]],\n",
    "        \"Spin\": [float(x) for x in df.iloc[1:-1, 3]],\n",
    "    }\n",
    ")\n",
    "inputs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_2 = pd.DataFrame({\"PCE\": [float(x) for x in df[\"PCE\"][1:-1]]})\n",
    "responses_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[[\"Donor %\", \"Conc.\", \"Spin\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inputs = pd.concat(\n",
    "    [inputs[[\"Donor %\", \"Conc.\", \"Spin\"]], inputs_2], axis=0\n",
    ")\n",
    "new_responses = pd.concat([responses, responses_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_source_list = []\n",
    "source_list = []\n",
    "sat_inputs_2, sat_source_list = doenut.add_higher_order_terms(\n",
    "    new_inputs, add_squares=True, add_interactions=True, column_list=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturated model: 9 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_selector = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "dataset = doenut.data.ModifiableDataSet(sat_inputs_2, new_responses).filter(\n",
    "    input_selector\n",
    ")\n",
    "\n",
    "model = doenut.models.AveragedModel(\n",
    "    dataset, scale_data=True, drop_duplicates=\"no\"\n",
    ")\n",
    "\n",
    "r2, q2 = model.r2, model.q2\n",
    "\n",
    "print(f\"R2 is {r2}, Q2 is {q2}\")\n",
    "doenut.plot.plot_summary_of_fit_small(r2, q2)\n",
    "doenut.plot.coeff_plot(\n",
    "    model.coeffs,\n",
    "    labels=list(dataset.get().inputs.columns),\n",
    "    errors=\"p95\",\n",
    "    normalise=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimised parsimonious model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_selector = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "dataset = doenut.data.ModifiableDataSet(sat_inputs_2, new_responses).filter(\n",
    "    input_selector\n",
    ")\n",
    "\n",
    "model = doenut.models.AveragedModel(\n",
    "    dataset, scale_data=True, drop_duplicates=\"no\"\n",
    ")\n",
    "\n",
    "r2, q2 = model.r2, model.q2\n",
    "\n",
    "print(f\"R2 is {r2}, Q2 is {q2}\")\n",
    "doenut.plot.plot_summary_of_fit_small(r2, q2)\n",
    "doenut.plot.coeff_plot(\n",
    "    model.coeffs,\n",
    "    labels=list(dataset.get().inputs.columns),\n",
    "    errors=\"p95\",\n",
    "    normalise=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "27 - 1 - 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| No. of terms | DoF   | term removed | factor removed | $R^2$  |  $Q^2$ |\n",
    "|--------------|-------|--------------|----------------|--------|--------|\n",
    "| 9            |   17  |              |                | 0.89   | -0.425 |\n",
    "| 8            |   18  |    8         | `Conc.*Spin`   | 0.887. | 0.44   |\n",
    "| 7            |   19  |    7         | `Donor*Spin`   | 0.88   | 0.535  |\n",
    "| 6            |   20  |    6         | `Donor* Conc`  | 0.871  | 0.695  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Optimising the devices. Using the best model that you have trained (as measured by Q2), find some conditions to optimise the devices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Plot a 4-D contour plot and read the values off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Donor %', 'Conc.', 'Spin'\n",
    "\n",
    "n_points = 60\n",
    "\n",
    "\n",
    "def my_function(df_1):\n",
    "    ## Put the two main factors that you're not plotting here\n",
    "    ## set them to sensible constant values\n",
    "\n",
    "    df_1[\"Donor %**2\"] = df_1[\"Donor %\"] * df_1[\"Donor %\"]\n",
    "    df_1[\"Conc.**2\"] = df_1[\"Conc.\"] * df_1[\"Conc.\"]\n",
    "    df_1[\"Spin**2\"] = df_1[\"Spin\"] * df_1[\"Spin\"]\n",
    "\n",
    "    return df_1\n",
    "\n",
    "\n",
    "c_key = \"Spin\"\n",
    "y_key = \"Conc.\"\n",
    "x_key = \"Donor %\"\n",
    "\n",
    "doenut.plot.four_D_contour_plot(\n",
    "    unscaled_model=model.model,\n",
    "    x_key=x_key,\n",
    "    y_key=y_key,\n",
    "    c_key=c_key,\n",
    "    x_limits=[inputs[x_key].min(), inputs[x_key].max()],\n",
    "    y_limits=[inputs[y_key].min(), inputs[y_key].max()],\n",
    "    constants=[500, 1500, 2500],\n",
    "    n_points=60,\n",
    "    my_function=my_function,\n",
    "    input_selector=[],\n",
    "    fig_label=\"Solar Cells\",\n",
    "    x_label=x_key,\n",
    "    y_label=y_key,\n",
    "    constant_label=c_key,\n",
    "    z_label=\"PCE\",\n",
    "    cmap=\"jet\",\n",
    "    num_of_z_levels=16,\n",
    "    z_limits=[0, 12],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2. Run the model on the input values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_5p2 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": {\"Donor %\": 20, \"Conc.\": 12, \"Spin\": 500},\n",
    "        \"B\": {\"Donor %\": 40, \"Conc.\": 16, \"Spin\": 1500},\n",
    "        \"C\": {\"Donor %\": 35, \"Conc.\": 22, \"Spin\": 1500},\n",
    "        \"D\": {\"Donor %\": 45, \"Conc.\": 18, \"Spin\": 2500},\n",
    "        \"E\": {\"Donor %\": 20, \"Conc.\": 17, \"Spin\": 2500},\n",
    "    }\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_5p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_5p2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_source_list = []\n",
    "source_list = []\n",
    "sat_inputs_q5, sat_source_list = doenut.add_higher_order_terms(\n",
    "    question_5p2,\n",
    "    add_squares=True,\n",
    "    add_interactions=True,\n",
    "    column_list=[],\n",
    ")\n",
    "\n",
    "results, _ = doenut.predict_from_model(\n",
    "    model.model, sat_inputs_q5, input_selector\n",
    ")\n",
    "letters = [x for x in question_5p2.index]\n",
    "[print(f\"{letters[i]}:\\t{results[i]}\") for i in range(len(letters))];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer C is above 7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
